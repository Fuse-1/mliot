##Importing all the required libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.naive_bayes import MultinomialNB,GaussianNB
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
import seaborn as sns

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split


from sklearn.metrics import accuracy_score
#Loading dataset
data = pd.read_csv("spam.csv",encoding='latin-1')
data.head()
#Droping unwanted columns
data = data.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1)


#Naming the columns
data = data.rename(columns = {'v1':'label','v2':'message'})
data.describe()
data.groupby('label').describe()
#Calculating the lenght of all the messages
data['lenght']=data['message'].apply(len)
data.head()


data.describe()


%matplotlib inline
#Plotting the lenght of ham and spam messages
data.hist(by='label',column='lenght',bins=30,figsize=[15,5])

#Converting our labels to numeric labels
# ham = 0 and spam = 1
data['label_num']=data.label.map({'ham':0,'spam':1})
data.head()

#loading our features and target to train
x = data.message
y = data.label_num

#Now we have 5572 labels for 5572 features
#spliting data into 75% train and 25% into test which is by default
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=11)
vect = CountVectorizer()

#converting features into numeric vector
X_train = vect.fit_transform(x_train)

#converting target into numeric vector
X_test = vect.transform(x_test)

#Loading all classifier
svc = SVC(kernel = 'linear')
mnb = MultinomialNB(alpha =0.2)
gnb = GaussianNB()
lr = LogisticRegression(solver='liblinear', penalty='l1')
rfc = RandomForestClassifier(n_estimators=100,random_state=11)
abc = AdaBoostClassifier(n_estimators =100,random_state=11)

#defining functions for training and testing data
def training(clf,x_train,Y_train):
clf.fit(x_train,Y_train)

#function for predicting labels
def predict(clf,X_test):
return clf.predict(X_test)

#defining a dictionary of classifier
classifier={'SVM': svc , 'MultinomialNB': mnb,'GaussianNB': gnb,'logistic': lr,'RandomForest': rfc,'Adaboost': abc}

#predict and storing score of each classifier
score = []
for n,c in classifier.items():
training(c,X_train.toarray(),y_train)
pred = predict(c,X_test.toarray())
score.append((n,[accuracy_score(y_test,pred,normalize=True)]))
score_df=pd.DataFrame.from_items(score,orient='index',columns=['scores'])

#Adding accuracy column
score_df['Accuracy (%)']=score_df['scores']*100
score_df
